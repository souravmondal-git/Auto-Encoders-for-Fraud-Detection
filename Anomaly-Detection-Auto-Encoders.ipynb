{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.2\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"creditcardfraud_normalised.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.935192</td>\n",
       "      <td>0.766490</td>\n",
       "      <td>0.881365</td>\n",
       "      <td>0.313023</td>\n",
       "      <td>0.763439</td>\n",
       "      <td>0.267669</td>\n",
       "      <td>0.266815</td>\n",
       "      <td>0.786444</td>\n",
       "      <td>0.475312</td>\n",
       "      <td>0.510600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561184</td>\n",
       "      <td>0.522992</td>\n",
       "      <td>0.663793</td>\n",
       "      <td>0.391253</td>\n",
       "      <td>0.585122</td>\n",
       "      <td>0.394557</td>\n",
       "      <td>0.418976</td>\n",
       "      <td>0.312697</td>\n",
       "      <td>0.005824</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.978542</td>\n",
       "      <td>0.770067</td>\n",
       "      <td>0.840298</td>\n",
       "      <td>0.271796</td>\n",
       "      <td>0.766120</td>\n",
       "      <td>0.262192</td>\n",
       "      <td>0.264875</td>\n",
       "      <td>0.786298</td>\n",
       "      <td>0.453981</td>\n",
       "      <td>0.505267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557840</td>\n",
       "      <td>0.480237</td>\n",
       "      <td>0.666938</td>\n",
       "      <td>0.336440</td>\n",
       "      <td>0.587290</td>\n",
       "      <td>0.446013</td>\n",
       "      <td>0.416345</td>\n",
       "      <td>0.313423</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.935217</td>\n",
       "      <td>0.753118</td>\n",
       "      <td>0.868141</td>\n",
       "      <td>0.268766</td>\n",
       "      <td>0.762329</td>\n",
       "      <td>0.281122</td>\n",
       "      <td>0.270177</td>\n",
       "      <td>0.788042</td>\n",
       "      <td>0.410603</td>\n",
       "      <td>0.513018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565477</td>\n",
       "      <td>0.546030</td>\n",
       "      <td>0.678939</td>\n",
       "      <td>0.289354</td>\n",
       "      <td>0.559515</td>\n",
       "      <td>0.402727</td>\n",
       "      <td>0.415489</td>\n",
       "      <td>0.311911</td>\n",
       "      <td>0.014739</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.941878</td>\n",
       "      <td>0.765304</td>\n",
       "      <td>0.868484</td>\n",
       "      <td>0.213661</td>\n",
       "      <td>0.765647</td>\n",
       "      <td>0.275559</td>\n",
       "      <td>0.266803</td>\n",
       "      <td>0.789434</td>\n",
       "      <td>0.414999</td>\n",
       "      <td>0.507585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559734</td>\n",
       "      <td>0.510277</td>\n",
       "      <td>0.662607</td>\n",
       "      <td>0.223826</td>\n",
       "      <td>0.614245</td>\n",
       "      <td>0.389197</td>\n",
       "      <td>0.417669</td>\n",
       "      <td>0.314371</td>\n",
       "      <td>0.004807</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.938617</td>\n",
       "      <td>0.776520</td>\n",
       "      <td>0.864251</td>\n",
       "      <td>0.269796</td>\n",
       "      <td>0.762975</td>\n",
       "      <td>0.263984</td>\n",
       "      <td>0.268968</td>\n",
       "      <td>0.782484</td>\n",
       "      <td>0.490950</td>\n",
       "      <td>0.524303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561327</td>\n",
       "      <td>0.547271</td>\n",
       "      <td>0.663392</td>\n",
       "      <td>0.401270</td>\n",
       "      <td>0.566343</td>\n",
       "      <td>0.507497</td>\n",
       "      <td>0.420561</td>\n",
       "      <td>0.317490</td>\n",
       "      <td>0.002724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>0.756448</td>\n",
       "      <td>0.873531</td>\n",
       "      <td>0.666991</td>\n",
       "      <td>0.160317</td>\n",
       "      <td>0.729603</td>\n",
       "      <td>0.236810</td>\n",
       "      <td>0.235393</td>\n",
       "      <td>0.863749</td>\n",
       "      <td>0.528729</td>\n",
       "      <td>0.598850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.564920</td>\n",
       "      <td>0.515249</td>\n",
       "      <td>0.680500</td>\n",
       "      <td>0.313600</td>\n",
       "      <td>0.658558</td>\n",
       "      <td>0.466291</td>\n",
       "      <td>0.433929</td>\n",
       "      <td>0.329840</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>0.945845</td>\n",
       "      <td>0.766677</td>\n",
       "      <td>0.872678</td>\n",
       "      <td>0.219189</td>\n",
       "      <td>0.771561</td>\n",
       "      <td>0.273661</td>\n",
       "      <td>0.265504</td>\n",
       "      <td>0.788548</td>\n",
       "      <td>0.482925</td>\n",
       "      <td>0.488530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.564933</td>\n",
       "      <td>0.553154</td>\n",
       "      <td>0.665619</td>\n",
       "      <td>0.245298</td>\n",
       "      <td>0.543855</td>\n",
       "      <td>0.360884</td>\n",
       "      <td>0.417775</td>\n",
       "      <td>0.312038</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>0.990905</td>\n",
       "      <td>0.764080</td>\n",
       "      <td>0.781102</td>\n",
       "      <td>0.227202</td>\n",
       "      <td>0.783425</td>\n",
       "      <td>0.293496</td>\n",
       "      <td>0.263547</td>\n",
       "      <td>0.792985</td>\n",
       "      <td>0.477677</td>\n",
       "      <td>0.498692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565220</td>\n",
       "      <td>0.537005</td>\n",
       "      <td>0.664877</td>\n",
       "      <td>0.468492</td>\n",
       "      <td>0.592823</td>\n",
       "      <td>0.411176</td>\n",
       "      <td>0.416593</td>\n",
       "      <td>0.312585</td>\n",
       "      <td>0.002642</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>0.954209</td>\n",
       "      <td>0.772856</td>\n",
       "      <td>0.849587</td>\n",
       "      <td>0.282508</td>\n",
       "      <td>0.763172</td>\n",
       "      <td>0.269291</td>\n",
       "      <td>0.261175</td>\n",
       "      <td>0.792671</td>\n",
       "      <td>0.476287</td>\n",
       "      <td>0.500464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565755</td>\n",
       "      <td>0.547353</td>\n",
       "      <td>0.663008</td>\n",
       "      <td>0.398836</td>\n",
       "      <td>0.545958</td>\n",
       "      <td>0.514746</td>\n",
       "      <td>0.418520</td>\n",
       "      <td>0.315245</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>0.949232</td>\n",
       "      <td>0.765256</td>\n",
       "      <td>0.849601</td>\n",
       "      <td>0.229488</td>\n",
       "      <td>0.765632</td>\n",
       "      <td>0.256488</td>\n",
       "      <td>0.274963</td>\n",
       "      <td>0.780938</td>\n",
       "      <td>0.479528</td>\n",
       "      <td>0.489782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565688</td>\n",
       "      <td>0.540031</td>\n",
       "      <td>0.671029</td>\n",
       "      <td>0.383420</td>\n",
       "      <td>0.551319</td>\n",
       "      <td>0.291786</td>\n",
       "      <td>0.416466</td>\n",
       "      <td>0.313401</td>\n",
       "      <td>0.008446</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0       0.935192  0.766490  0.881365  0.313023  0.763439  0.267669  0.266815   \n",
       "1       0.978542  0.770067  0.840298  0.271796  0.766120  0.262192  0.264875   \n",
       "2       0.935217  0.753118  0.868141  0.268766  0.762329  0.281122  0.270177   \n",
       "3       0.941878  0.765304  0.868484  0.213661  0.765647  0.275559  0.266803   \n",
       "4       0.938617  0.776520  0.864251  0.269796  0.762975  0.263984  0.268968   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284802  0.756448  0.873531  0.666991  0.160317  0.729603  0.236810  0.235393   \n",
       "284803  0.945845  0.766677  0.872678  0.219189  0.771561  0.273661  0.265504   \n",
       "284804  0.990905  0.764080  0.781102  0.227202  0.783425  0.293496  0.263547   \n",
       "284805  0.954209  0.772856  0.849587  0.282508  0.763172  0.269291  0.261175   \n",
       "284806  0.949232  0.765256  0.849601  0.229488  0.765632  0.256488  0.274963   \n",
       "\n",
       "              V8        V9       V10  ...       V21       V22       V23  \\\n",
       "0       0.786444  0.475312  0.510600  ...  0.561184  0.522992  0.663793   \n",
       "1       0.786298  0.453981  0.505267  ...  0.557840  0.480237  0.666938   \n",
       "2       0.788042  0.410603  0.513018  ...  0.565477  0.546030  0.678939   \n",
       "3       0.789434  0.414999  0.507585  ...  0.559734  0.510277  0.662607   \n",
       "4       0.782484  0.490950  0.524303  ...  0.561327  0.547271  0.663392   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "284802  0.863749  0.528729  0.598850  ...  0.564920  0.515249  0.680500   \n",
       "284803  0.788548  0.482925  0.488530  ...  0.564933  0.553154  0.665619   \n",
       "284804  0.792985  0.477677  0.498692  ...  0.565220  0.537005  0.664877   \n",
       "284805  0.792671  0.476287  0.500464  ...  0.565755  0.547353  0.663008   \n",
       "284806  0.780938  0.479528  0.489782  ...  0.565688  0.540031  0.671029   \n",
       "\n",
       "             V24       V25       V26       V27       V28    Amount  class  \n",
       "0       0.391253  0.585122  0.394557  0.418976  0.312697  0.005824      0  \n",
       "1       0.336440  0.587290  0.446013  0.416345  0.313423  0.000105      0  \n",
       "2       0.289354  0.559515  0.402727  0.415489  0.311911  0.014739      0  \n",
       "3       0.223826  0.614245  0.389197  0.417669  0.314371  0.004807      0  \n",
       "4       0.401270  0.566343  0.507497  0.420561  0.317490  0.002724      0  \n",
       "...          ...       ...       ...       ...       ...       ...    ...  \n",
       "284802  0.313600  0.658558  0.466291  0.433929  0.329840  0.000030      0  \n",
       "284803  0.245298  0.543855  0.360884  0.417775  0.312038  0.000965      0  \n",
       "284804  0.468492  0.592823  0.411176  0.416593  0.312585  0.002642      0  \n",
       "284805  0.398836  0.545958  0.514746  0.418520  0.315245  0.000389      0  \n",
       "284806  0.383420  0.551319  0.291786  0.416466  0.313401  0.008446      0  \n",
       "\n",
       "[284807 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n",
       "       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n",
       "       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['class']\n",
    "X=df.drop(columns=['class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_validate, y_train2, y_validate = train_test_split(X_train, y_train, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 29)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199364, 29)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 29)                870       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                480       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 29)                493       \n",
      "=================================================================\n",
      "Total params: 2,221\n",
      "Trainable params: 2,221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train2.shape[1]\n",
    "BATCH_SIZE = 200\n",
    "EPOCHS = 100\n",
    "\n",
    "model = Sequential([\n",
    "    \n",
    "    Dense(input_dim, activation='elu', input_shape=(input_dim,)),\n",
    "    Dense(16, activation='elu'),\n",
    "    Dense(8, activation='elu'),\n",
    "    Dense(4, activation='elu'),\n",
    "    \n",
    "    Dense(2, activation='elu'),\n",
    "    \n",
    "    Dense(4, activation='elu'),\n",
    "    Dense(8, activation='elu'),\n",
    "    Dense(16, activation='elu'),\n",
    "    Dense(input_dim, activation='elu')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard module is not an IPython extension.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# current date and time\n",
    "yyyymmddHHMM = datetime.now().strftime('%Y%m%d%H%M')\n",
    "\n",
    "# new folder for a new run\n",
    "log_subdir = f'{yyyymmddHHMM}_batch{BATCH_SIZE}_layers{len(model.layers)}'\n",
    "\n",
    "# define our early stopping\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=10,\n",
    "    verbose=1, \n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "save_model = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='autoencoder_best_weights.hdf5',\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    verbose=0,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "    f'logs/{log_subdir}',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    update_freq='batch'\n",
    ")\n",
    "\n",
    "# callbacks argument only takes a list\n",
    "cb = [early_stop, save_model, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 139554 samples, validate on 59810 samples\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "139554/139554 [==============================] - 4s 32us/sample - loss: 0.0137 - acc: 0.8770 - val_loss: 0.0017 - val_acc: 0.9916\n",
      "Epoch 2/100\n",
      "139554/139554 [==============================] - 3s 23us/sample - loss: 0.0017 - acc: 0.9921 - val_loss: 0.0017 - val_acc: 0.9916\n",
      "Epoch 3/100\n",
      "139554/139554 [==============================] - 3s 23us/sample - loss: 0.0017 - acc: 0.9921 - val_loss: 0.0016 - val_acc: 0.9916\n",
      "Epoch 4/100\n",
      "139554/139554 [==============================] - 3s 23us/sample - loss: 0.0016 - acc: 0.9921 - val_loss: 0.0015 - val_acc: 0.9916\n",
      "Epoch 5/100\n",
      "139554/139554 [==============================] - 3s 23us/sample - loss: 0.0015 - acc: 0.9921 - val_loss: 0.0015 - val_acc: 0.9916\n",
      "Epoch 6/100\n",
      "139554/139554 [==============================] - 3s 23us/sample - loss: 0.0015 - acc: 0.9921 - val_loss: 0.0015 - val_acc: 0.9916\n",
      "Epoch 7/100\n",
      "139554/139554 [==============================] - 3s 23us/sample - loss: 0.0014 - acc: 0.9921 - val_loss: 0.0014 - val_acc: 0.9916\n",
      "Epoch 8/100\n",
      "139554/139554 [==============================] - 3s 23us/sample - loss: 0.0014 - acc: 0.9921 - val_loss: 0.0013 - val_acc: 0.9916\n",
      "Epoch 9/100\n",
      "139554/139554 [==============================] - 3s 23us/sample - loss: 0.0013 - acc: 0.9921 - val_loss: 0.0013 - val_acc: 0.9916\n",
      "Epoch 10/100\n",
      "139554/139554 [==============================] - 3s 23us/sample - loss: 0.0013 - acc: 0.9921 - val_loss: 0.0013 - val_acc: 0.9916\n",
      "Epoch 11/100\n",
      "139554/139554 [==============================] - 3s 23us/sample - loss: 0.0013 - acc: 0.9921 - val_loss: 0.0013 - val_acc: 0.9916\n",
      "Epoch 12/100\n",
      "139554/139554 [==============================] - 3s 23us/sample - loss: 0.0013 - acc: 0.9921 - val_loss: 0.0013 - val_acc: 0.9916\n",
      "Epoch 13/100\n",
      "139554/139554 [==============================] - 3s 23us/sample - loss: 0.0013 - acc: 0.9921 - val_loss: 0.0013 - val_acc: 0.9916\n",
      "Epoch 14/100\n",
      "139554/139554 [==============================] - 4s 27us/sample - loss: 0.0013 - acc: 0.9921 - val_loss: 0.0013 - val_acc: 0.9916\n",
      "Epoch 15/100\n",
      "139554/139554 [==============================] - 3s 23us/sample - loss: 0.0013 - acc: 0.9921 - val_loss: 0.0013 - val_acc: 0.9916\n",
      "Epoch 16/100\n",
      "139554/139554 [==============================] - 3s 23us/sample - loss: 0.0013 - acc: 0.9921 - val_loss: 0.0013 - val_acc: 0.9916\n",
      "Epoch 17/100\n",
      "139554/139554 [==============================] - 3s 23us/sample - loss: 0.0013 - acc: 0.9921 - val_loss: 0.0013 - val_acc: 0.9916\n",
      "Epoch 18/100\n",
      "139554/139554 [==============================] - 3s 24us/sample - loss: 0.0013 - acc: 0.9921 - val_loss: 0.0013 - val_acc: 0.9916\n",
      "Epoch 19/100\n",
      "139200/139554 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9921Restoring model weights from the end of the best epoch.\n",
      "139554/139554 [==============================] - 3s 24us/sample - loss: 0.0013 - acc: 0.9921 - val_loss: 0.0013 - val_acc: 0.9916\n",
      "Epoch 00019: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train2, X_train2,\n",
    "    shuffle=True,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=cb,\n",
    "    validation_data=(X_validate, X_validate)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.mean(np.power(X_test - reconstructions, 2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df=pd.DataFrame(X_test, index=X_test.index, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_df=pd.DataFrame(y_test, index=X_test.index, columns=['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean_col=list(y_test_df[y_test_df['class']==0].index)\n",
    "X_test_cleanDF=X_test_df.loc[test_clean_col,:]\n",
    "test_fraud_col=list(y_test_df[y_test_df['class']==1].index)\n",
    "X_test_fraudDF=X_test_df.loc[test_fraud_col,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_reconstruction=model.predict(np.array(X_test_cleanDF))\n",
    "fraud_reconstruction=model.predict(np.array(X_test_fraudDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_clean = np.mean(np.power(np.array(X_test_cleanDF) - clean_reconstruction, 2), axis=1)\n",
    "mse_fraud = np.mean(np.power(np.array(X_test_fraudDF) - fraud_reconstruction, 2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.hist(mse_clean, bins=100, density=True, label=\"clean\", alpha=.6, color=\"green\")\n",
    "ax.hist(mse_fraud, bins=100, density=True, label=\"fraud\", alpha=.6, color=\"red\")\n",
    "\n",
    "plt.title(\"(Normalized) Distribution of the Reconstruction Loss\")\n",
    "plt.xlim([-0.01,0.1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "577.8760162601626"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['class']==0].shape[0]/df[df['class']==1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop('class',axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.35192e-01, 7.66490e-01, 8.81365e-01, ..., 4.18976e-01,\n",
       "        3.12697e-01, 5.82400e-03],\n",
       "       [9.78542e-01, 7.70067e-01, 8.40298e-01, ..., 4.16345e-01,\n",
       "        3.13423e-01, 1.05000e-04],\n",
       "       [9.35217e-01, 7.53118e-01, 8.68141e-01, ..., 4.15489e-01,\n",
       "        3.11911e-01, 1.47390e-02],\n",
       "       ...,\n",
       "       [9.90905e-01, 7.64080e-01, 7.81102e-01, ..., 4.16593e-01,\n",
       "        3.12585e-01, 2.64200e-03],\n",
       "       [9.54209e-01, 7.72856e-01, 8.49587e-01, ..., 4.18520e-01,\n",
       "        3.15245e-01, 3.89000e-04],\n",
       "       [9.49232e-01, 7.65256e-01, 8.49601e-01, ..., 4.16466e-01,\n",
       "        3.13401e-01, 8.44600e-03]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding outliers using modified z-score\n",
    "THRESHOLD = 3\n",
    "\n",
    "def mz_score(x):\n",
    "    med = np.median(x)\n",
    "    abs_med = np.abs(x - med)\n",
    "    mad = np.median(abs_med)\n",
    "    \n",
    "    return 0.6745 * abs_med/ mad\n",
    " \n",
    "\n",
    "MZ_scores = mz_score(mse)\n",
    "outliers = MZ_scores > THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 4,024 outliers in a total of 85,443 transactions [4.71%].\n"
     ]
    }
   ],
   "source": [
    "print(f\"Detected {np.sum(outliers):,} outliers in a total of {np.size(z_scores):,} transactions [{np.sum(outliers)/np.size(z_scores):.2%}].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (confusion_matrix, \n",
    "                             precision_recall_curve)\n",
    "\n",
    "# get (mis)classification\n",
    "cm = confusion_matrix(y_test, outliers)\n",
    "\n",
    "# true/false positives/negatives\n",
    "(tn, fp, \n",
    " fn, tp) = cm.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classifications using the MAD method with threshold=3 are as follows:\n",
      "[[81400  3899]\n",
      " [   19   125]]\n",
      "\n",
      "percentage of transactions labeled as fraud that were correct (precision): 125/(3899+125) = 3.11%\n",
      "percentage of fraudulent transactions were caught succesfully (recall):    125/(19+125) = 86.81%\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"The classifications using the MAD method with threshold={THRESHOLD} are as follows:\n",
    "{cm}\n",
    "\n",
    "percentage of transactions labeled as fraud that were correct (precision): {tp}/({fp}+{tp}) = {tp/(fp+tp):.2%}\n",
    "percentage of fraudulent transactions were caught succesfully (recall):    {tp}/({fn}+{tp}) = {tp/(fn+tp):.2%}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9111728791271488\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(y_test, outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
